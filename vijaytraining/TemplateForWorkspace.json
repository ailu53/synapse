{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "vijaytraining"
		},
		"GitHub1_credential": {
			"type": "secureString",
			"metadata": "Secure string for 'credential' of 'GitHub1'"
		},
		"MySql1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'MySql1'"
		},
		"vijaytraining-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'vijaytraining-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:vijaytraining.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"GitHub1_properties_typeProperties_username": {
			"type": "string",
			"defaultValue": "ailu53"
		},
		"vijaytraining-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://synapsdl.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/copy_all_trables')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "look for tables",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "MySqlSource",
								"query": "show tables;"
							},
							"dataset": {
								"referenceName": "MySqlTable1",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "foreach schema table",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "look for tables",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('look for tables').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "copy each table",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "MySqlSource",
											"query": {
												"value": "@{concat('SELECT * FROM',item().Tables_in_hr)}",
												"type": "Expression"
											}
										},
										"sink": {
											"type": "ParquetSink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											},
											"formatSettings": {
												"type": "ParquetWriteSettings"
											}
										},
										"enableStaging": false,
										"translator": {
											"type": "TabularTranslator",
											"typeConversion": true,
											"typeConversionSettings": {
												"allowDataTruncation": true,
												"treatBooleanAsNumber": false
											}
										}
									},
									"inputs": [
										{
											"referenceName": "mysqlcopy",
											"type": "DatasetReference",
											"parameters": {}
										}
									],
									"outputs": [
										{
											"referenceName": "parquetTables",
											"type": "DatasetReference",
											"parameters": {
												"tablename": {
													"value": "@item().Tables_in_hr",
													"type": "Expression"
												}
											}
										}
									]
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/MySqlTable1')]",
				"[concat(variables('workspaceId'), '/datasets/mysqlcopy')]",
				"[concat(variables('workspaceId'), '/datasets/parquetTables')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MySqlTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "MySql1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "MySqlTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/MySql1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mysqlcopy')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "MySql1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "MySqlTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/MySql1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/parquetTables')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "vijaytraining-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"tablename": {
						"type": "string",
						"defaultValue": "@item.Tables_in_hr"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@{concat(dataset().tablename,'.parquet')}",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@{concat('bronze',dataset().tablename)}",
							"type": "Expression"
						},
						"fileSystem": "mrk"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/vijaytraining-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GitHub1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "GitHub",
				"typeProperties": {
					"username": "[parameters('GitHub1_properties_typeProperties_username')]",
					"credential": {
						"type": "SecureString",
						"value": "[parameters('GitHub1_credential')]"
					},
					"encryptedCredential": "ew0KICAiVmVyc2lvbiI6ICIyMDE3LTExLTMwIiwNCiAgIlByb3RlY3Rpb25Nb2RlIjogIktleSIsDQogICJTZWNyZXRDb250ZW50VHlwZSI6ICJQbGFpbnRleHQiLA0KICAiQ3JlZGVudGlhbElkIjogIlNZTkFQU0VAODNGMDc4NDEtNzI2Ni00RUZGLTk0NTctOUREMjhFOTFFMTJCXzNlZTYwYTJhLWZhZWEtNDJlMC04MjY1LTE1OGI2YzJjMTk1YiINCn0="
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MySql1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "MySql",
				"typeProperties": {
					"connectionString": "[parameters('MySql1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "IntegrationRuntimemysql1",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/IntegrationRuntimemysql1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/vijaytraining-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('vijaytraining-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/vijaytraining-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('vijaytraining-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntimemysql1')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AirQuality')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "trainingsparkpl",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6081f79a-0a81-4f2e-b3ca-6b8695b1818b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/9260bc06-d410-4a17-9670-50b0aaee72ad/resourceGroups/Synaps_Training/providers/Microsoft.Synapse/workspaces/vijaytraining/bigDataPools/trainingsparkpl",
						"name": "trainingsparkpl",
						"type": "Spark",
						"endpoint": "https://vijaytraining.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/trainingsparkpl",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"import pyspark.pandas as ps\r\n",
							"from pyspark.sql.functions import lit\r\n",
							"\r\n",
							"# Define connection properties\r\n",
							"url = \"jdbc:sqlserver://vijaytraining.sql.azuresynapse.net:1433;database=trainingsqlpl;user=sqladminuser;password=Vijay1234;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30\"\r\n",
							"properties = {\r\n",
							"    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
							"}\r\n",
							"\r\n",
							"\r\n",
							"file_path = \"abfss://vijay@synapsdl.dfs.core.windows.net/*.ndjson\"\r\n",
							"\r\n",
							"# Read the NDJSON file into a DataFrame\r\n",
							"df = spark.read.json(file_path)\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"df = df.withColumn(\"date_utc\", lit(df[\"date.utc\"])) \\\r\n",
							"       .withColumn(\"date_local\", lit(df[\"date.local\"])) \\\r\n",
							"       .drop(\"date\")\r\n",
							"\r\n",
							"df = df.withColumn(\"averagingPeriod_unit\", df[\"averagingPeriod.unit\"]) \\\r\n",
							"       .withColumn(\"averagingPeriod_value\", df[\"averagingPeriod.value\"]) \\\r\n",
							"       .drop(\"averagingPeriod\")\r\n",
							"df = df.withColumn(\"coordinates_lat\", df[\"coordinates.latitude\"]) \\\r\n",
							"       .withColumn(\"coordinates_long\", df[\"coordinates.longitude\"]) \\\r\n",
							"       .drop(\"coordinates\")\r\n",
							"# Show the DataFrame to verify the transformation\r\n",
							"df=df.drop('attribution')\r\n",
							"df.toPandas()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 281
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"\r\n",
							"# Define the target table name in your dedicated SQL pool\r\n",
							"table_name = \"air_quality\"\r\n",
							"#df = df.withColumn(\"city\", df[\"city\"].cast(\"string\"))\r\n",
							"# Write the DataFrame to the dedicated SQL pool\r\n",
							"df.write.jdbc(url=url, table=table_name, mode=\"append\", properties=properties)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 282
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"path = \"abfss://vijay@synapsdl.dfs.core.windows.net/countries.csv\"\r\n",
							"\r\n",
							"df = spark.read.option(\"header\", True).csv(path)\r\n",
							"df.show()\r\n",
							"df.write.jdbc(url=url, table='country', mode=\"append\", properties=properties)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"\r\n",
							"df_air = spark.read.jdbc(url=url, table=\"air_quality\", properties=properties)\r\n",
							"df_country = spark.read.jdbc(url=url, table=\"country\", properties=properties)\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Assuming we're filtering df1 based on values in 'id' column of df2\r\n",
							"\r\n",
							"# Collect unique 'id' values from df2 into a list\r\n",
							"country_to_filter = df_country.select(\"country_code\").distinct().rdd.flatMap(lambda x: x).collect()\r\n",
							"\r\n",
							"# Filter df1 where 'id' is in the collected list\r\n",
							"filtered_df_air = df_air.filter(df_air[\"country\"].isin(country_to_filter))\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Filter for required parameters\r\n",
							"filtered_df_air.select('parameter').distinct().show()\r\n",
							"param_list=['PM25', 'PM10', 'O3', 'NO2', 'CO']\r\n",
							"filtered_df_air1 = filtered_df_air.filter(filtered_df_air[\"parameter\"].isin(param_list))"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"filtered_df_air1.select('parameter').distinct().show()\r\n",
							"filtered_df_air2.dtypes"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"filtered_df_air1 = filtered_df_air.select('city','country','parameter','unit','value','date_utc')\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"filtered_df_air2=filtered_df_air1.filter(filtered_df_air1['value'] > 0)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"filtered_df_air2.filter(filtered_df_air1['value'] < 0).show()"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"filtered_df_air2.select('date_utc').show(1)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import col, to_timestamp, avg, expr\r\n",
							"from pyspark.sql.window import Window\r\n",
							"\r\n",
							"# Ensure 'date_utc' is a datetime type\r\n",
							"#filtered_df_air2 = filtered_df_air2.withColumn(\"date_utc\", to_timestamp(col(\"date_utc\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z''\"))\r\n",
							"filtered_df_air2 = filtered_df_air2.withColumn('date_utc', filtered_df_air2.date_utc.cast('timestamp'))\r\n",
							"filtered_df_air2 = filtered_df_air2.withColumn('value', filtered_df_air2.value.cast('int'))"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Define the window specification for a 24-hour rolling window\r\n",
							"#window_spec = Window.partitionBy(\"city\", \"parameter\").orderBy(\"date_utc\").rangeBetween(-86400, 0)\r\n",
							"\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import expr\r\n",
							"from pyspark.sql import functions as F\r\n",
							"days = lambda i: i * 86400\r\n",
							"window_spec = Window \\\r\n",
							"    .partitionBy(\"city\", \"parameter\") \\\r\n",
							"    .orderBy(F.col(\"date_utc\").cast('long')) \\\r\n",
							"    .rangeBetween(-days(1), 0)\r\n",
							"\r\n",
							"\r\n",
							"    #orderBy(F.col(\"timestampGMT\").cast('long'))"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Calculate the rolling average\r\n",
							"df_rolling_avg = filtered_df_air2.withColumn(\"24hr_rolling_avg\", avg(\"value\").over(window_spec))\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 290
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Writing Rolling Avg to DB\r\n",
							"\r\n",
							"\r\n",
							"df_rolling_avg.write.jdbc(url=url, table='air_roll_avg', mode=\"append\", properties=properties)"
						],
						"outputs": [],
						"execution_count": 294
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_rolling_avg.show()"
						],
						"outputs": [],
						"execution_count": 303
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# AQI calculation function\r\n",
							"from pyspark.sql.functions import udf, col\r\n",
							"from pyspark.sql.types import DoubleType\r\n",
							"def calculate_aqi(pm_obs, breakpoint_low, breakpoint_high, aqi_low, aqi_high):\r\n",
							"    aqi = ((aqi_high - aqi_low) / (breakpoint_high - breakpoint_low)) * (pm_obs - breakpoint_low) + aqi_low\r\n",
							"    return aqi\r\n",
							"\r\n",
							"# Register the function as a UDF\r\n",
							"calculate_aqi_udf = udf(calculate_aqi, DoubleType())\r\n",
							"\r\n",
							"\r\n",
							"# Define the AQI breakpoints and AQI values for PM2.5\r\n",
							"breakpoint_low_PM25 = 12.1\r\n",
							"breakpoint_high_PM25 = 35.4\r\n",
							"aqi_low_PM25 = 51\r\n",
							"aqi_high_PM25 = 100\r\n",
							"\r\n",
							"\r\n",
							"# Define the AQI breakpoints and AQI values for PM10\r\n",
							"breakpoint_low_PM10 = 55.0\r\n",
							"breakpoint_high_PM10 = 154.0\r\n",
							"aqi_low_PM10= 51\r\n",
							"aqi_high_PM10 = 100\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 314
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_rolling_avg_PM25=df_rolling_avg.filter(df_rolling_avg['parameter'] == 'PM25')\r\n",
							"df_rolling_avg_PM10=df_rolling_avg.filter(df_rolling_avg['parameter'] == 'PM10')"
						],
						"outputs": [],
						"execution_count": 311
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Apply the UDF to calculate the AQI for PM2.5\r\n",
							"df_aqi_pm25 = df_rolling_avg_PM25.withColumn(\"aqi_pm25\", calculate_aqi_udf(\r\n",
							"    col(\"24hr_rolling_avg\"), \r\n",
							"    lit(breakpoint_low_PM25), \r\n",
							"    lit(breakpoint_high_PM25), \r\n",
							"    lit(aqi_low_PM25), \r\n",
							"    lit(aqi_high_PM25)\r\n",
							"))"
						],
						"outputs": [],
						"execution_count": 312
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_aqi_pm25.show()"
						],
						"outputs": [],
						"execution_count": 317
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_aqi_pm10 = df_rolling_avg_PM10.withColumn(\"aqi_pm10\", calculate_aqi_udf(\r\n",
							"    col(\"24hr_rolling_avg\"), \r\n",
							"    lit(breakpoint_low_PM10), \r\n",
							"    lit(breakpoint_high_PM10), \r\n",
							"    lit(aqi_low_PM10), \r\n",
							"    lit(aqi_high_PM10)\r\n",
							"))"
						],
						"outputs": [],
						"execution_count": 318
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_aqi_pm10.show()"
						],
						"outputs": [],
						"execution_count": 319
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Loading the AQI tables into the DB.\r\n",
							"df_aqi_pm10.write.jdbc(url=url, table='air_aqi_pm10', mode=\"append\", properties=properties)\r\n",
							"df_aqi_pm25.write.jdbc(url=url, table='air_aqi_pm25', mode=\"append\", properties=properties)"
						],
						"outputs": [],
						"execution_count": 320
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "trainingsparkpl",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0958c701-36eb-413a-911e-a4440c57ceb1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/9260bc06-d410-4a17-9670-50b0aaee72ad/resourceGroups/Synaps_Training/providers/Microsoft.Synapse/workspaces/vijaytraining/bigDataPools/trainingsparkpl",
						"name": "trainingsparkpl",
						"type": "Spark",
						"endpoint": "https://vijaytraining.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/trainingsparkpl",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"import pyspark.pandas as ps\r\n",
							"from pyspark.sql.functions import lit\r\n",
							"\r\n",
							"# Define connection properties\r\n",
							"url = \"jdbc:sqlserver://vijaytraining.sql.azuresynapse.net:1433;database=trainingsqlpl;user=sqladminuser;password=Vijay1234;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30\"\r\n",
							"properties = {\r\n",
							"    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
							"}\r\n",
							"\r\n",
							"\r\n",
							"file_path = \"abfss://vijay@synapsdl.dfs.core.windows.net/*.ndjson\"\r\n",
							"\r\n",
							"# Read the NDJSON file into a DataFrame\r\n",
							"df = spark.read.json(file_path)\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"df = df.withColumn(\"date_utc\", lit(df[\"date.utc\"])) \\\r\n",
							"       .withColumn(\"date_local\", lit(df[\"date.local\"])) \\\r\n",
							"       .drop(\"date\")\r\n",
							"\r\n",
							"df = df.withColumn(\"averagingPeriod_unit\", df[\"averagingPeriod.unit\"]) \\\r\n",
							"       .withColumn(\"averagingPeriod_value\", df[\"averagingPeriod.value\"]) \\\r\n",
							"       .drop(\"averagingPeriod\")\r\n",
							"df = df.withColumn(\"coordinates_lat\", df[\"coordinates.latitude\"]) \\\r\n",
							"       .withColumn(\"coordinates_long\", df[\"coordinates.longitude\"]) \\\r\n",
							"       .drop(\"coordinates\")\r\n",
							"# Show the DataFrame to verify the transformation\r\n",
							"df=df.drop('attribution')\r\n",
							"df.toPandas()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 281
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"\r\n",
							"# Define the target table name in your dedicated SQL pool\r\n",
							"table_name = \"air_quality\"\r\n",
							"#df = df.withColumn(\"city\", df[\"city\"].cast(\"string\"))\r\n",
							"# Write the DataFrame to the dedicated SQL pool\r\n",
							"df.write.jdbc(url=url, table=table_name, mode=\"append\", properties=properties)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 282
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"path = \"abfss://vijay@synapsdl.dfs.core.windows.net/countries.csv\"\r\n",
							"\r\n",
							"df = spark.read.option(\"header\", True).csv(path)\r\n",
							"df.show()\r\n",
							"df.write.jdbc(url=url, table='country', mode=\"append\", properties=properties)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"\r\n",
							"df_air = spark.read.jdbc(url=url, table=\"air_quality\", properties=properties)\r\n",
							"df_country = spark.read.jdbc(url=url, table=\"country\", properties=properties)\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Assuming we're filtering df1 based on values in 'id' column of df2\r\n",
							"\r\n",
							"# Collect unique 'id' values from df2 into a list\r\n",
							"country_to_filter = df_country.select(\"country_code\").distinct().rdd.flatMap(lambda x: x).collect()\r\n",
							"\r\n",
							"# Filter df1 where 'id' is in the collected list\r\n",
							"filtered_df_air = df_air.filter(df_air[\"country\"].isin(country_to_filter))\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Filter for required parameters\r\n",
							"filtered_df_air.select('parameter').distinct().show()\r\n",
							"param_list=['PM25', 'PM10', 'O3', 'NO2', 'CO']\r\n",
							"filtered_df_air1 = filtered_df_air.filter(filtered_df_air[\"parameter\"].isin(param_list))"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"filtered_df_air1.select('parameter').distinct().show()\r\n",
							"filtered_df_air2.dtypes"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"filtered_df_air1 = filtered_df_air.select('city','country','parameter','unit','value','date_utc')\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"filtered_df_air2=filtered_df_air1.filter(filtered_df_air1['value'] > 0)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"filtered_df_air2.filter(filtered_df_air1['value'] < 0).show()"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"filtered_df_air2.select('date_utc').show(1)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import col, to_timestamp, avg, expr\r\n",
							"from pyspark.sql.window import Window\r\n",
							"\r\n",
							"# Ensure 'date_utc' is a datetime type\r\n",
							"#filtered_df_air2 = filtered_df_air2.withColumn(\"date_utc\", to_timestamp(col(\"date_utc\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z''\"))\r\n",
							"filtered_df_air2 = filtered_df_air2.withColumn('date_utc', filtered_df_air2.date_utc.cast('timestamp'))\r\n",
							"filtered_df_air2 = filtered_df_air2.withColumn('value', filtered_df_air2.value.cast('int'))"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Define the window specification for a 24-hour rolling window\r\n",
							"#window_spec = Window.partitionBy(\"city\", \"parameter\").orderBy(\"date_utc\").rangeBetween(-86400, 0)\r\n",
							"\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import expr\r\n",
							"from pyspark.sql import functions as F\r\n",
							"days = lambda i: i * 86400\r\n",
							"window_spec = Window \\\r\n",
							"    .partitionBy(\"city\", \"parameter\") \\\r\n",
							"    .orderBy(F.col(\"date_utc\").cast('long')) \\\r\n",
							"    .rangeBetween(-days(1), 0)\r\n",
							"\r\n",
							"\r\n",
							"    #orderBy(F.col(\"timestampGMT\").cast('long'))"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Calculate the rolling average\r\n",
							"df_rolling_avg = filtered_df_air2.withColumn(\"24hr_rolling_avg\", avg(\"value\").over(window_spec))\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 290
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Writing Rolling Avg to DB\r\n",
							"\r\n",
							"\r\n",
							"df_rolling_avg.write.jdbc(url=url, table='air_roll_avg', mode=\"append\", properties=properties)"
						],
						"outputs": [],
						"execution_count": 294
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_rolling_avg.show()"
						],
						"outputs": [],
						"execution_count": 303
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# AQI calculation function\r\n",
							"from pyspark.sql.functions import udf, col\r\n",
							"from pyspark.sql.types import DoubleType\r\n",
							"def calculate_aqi(pm_obs, breakpoint_low, breakpoint_high, aqi_low, aqi_high):\r\n",
							"    aqi = ((aqi_high - aqi_low) / (breakpoint_high - breakpoint_low)) * (pm_obs - breakpoint_low) + aqi_low\r\n",
							"    return aqi\r\n",
							"\r\n",
							"# Register the function as a UDF\r\n",
							"calculate_aqi_udf = udf(calculate_aqi, DoubleType())\r\n",
							"\r\n",
							"\r\n",
							"# Define the AQI breakpoints and AQI values for PM2.5\r\n",
							"breakpoint_low_PM25 = 12.1\r\n",
							"breakpoint_high_PM25 = 35.4\r\n",
							"aqi_low_PM25 = 51\r\n",
							"aqi_high_PM25 = 100\r\n",
							"\r\n",
							"\r\n",
							"# Define the AQI breakpoints and AQI values for PM10\r\n",
							"breakpoint_low_PM10 = 55.0\r\n",
							"breakpoint_high_PM10 = 154.0\r\n",
							"aqi_low_PM10= 51\r\n",
							"aqi_high_PM10 = 100\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 314
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_rolling_avg_PM25=df_rolling_avg.filter(df_rolling_avg['parameter'] == 'PM25')\r\n",
							"df_rolling_avg_PM10=df_rolling_avg.filter(df_rolling_avg['parameter'] == 'PM10')"
						],
						"outputs": [],
						"execution_count": 311
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Apply the UDF to calculate the AQI for PM2.5\r\n",
							"df_aqi_pm25 = df_rolling_avg_PM25.withColumn(\"aqi_pm25\", calculate_aqi_udf(\r\n",
							"    col(\"24hr_rolling_avg\"), \r\n",
							"    lit(breakpoint_low_PM25), \r\n",
							"    lit(breakpoint_high_PM25), \r\n",
							"    lit(aqi_low_PM25), \r\n",
							"    lit(aqi_high_PM25)\r\n",
							"))"
						],
						"outputs": [],
						"execution_count": 312
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_aqi_pm25.show()"
						],
						"outputs": [],
						"execution_count": 317
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_aqi_pm10 = df_rolling_avg_PM10.withColumn(\"aqi_pm10\", calculate_aqi_udf(\r\n",
							"    col(\"24hr_rolling_avg\"), \r\n",
							"    lit(breakpoint_low_PM10), \r\n",
							"    lit(breakpoint_high_PM10), \r\n",
							"    lit(aqi_low_PM10), \r\n",
							"    lit(aqi_high_PM10)\r\n",
							"))"
						],
						"outputs": [],
						"execution_count": 318
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_aqi_pm10.show()"
						],
						"outputs": [],
						"execution_count": 319
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Loading the AQI tables into the DB.\r\n",
							"df_aqi_pm10.write.jdbc(url=url, table='air_aqi_pm10', mode=\"append\", properties=properties)\r\n",
							"df_aqi_pm25.write.jdbc(url=url, table='air_aqi_pm25', mode=\"append\", properties=properties)"
						],
						"outputs": [],
						"execution_count": 320
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/trainingsparkpl')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 5
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "centralindia"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/trainingsqlpl')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "centralindia"
		}
	]
}